# Small Language Model
A 14 milllion parameter Language model trained on PyTorch's WikiText-2 dataset adapted from PyTorch's Transformer tutorial. 

The dataset was extracted from wikipedia articles. It vocabulary is 28782 words which are around the length of the English Language. 
The training set is about 2 million tokens and the Testing Set is about 200,000 tokens. 
